{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cd17cd",
   "metadata": {},
   "source": [
    "## Login to hugging face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0970f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8597943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abhishekdey\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "HF_TOKEN_READ = os.getenv(\"HF_TOKEN_READ\")\n",
    "api = HfApi(token=HF_TOKEN_READ)\n",
    "user = api.whoami()\n",
    "\n",
    "print(user['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de529ac1",
   "metadata": {},
   "source": [
    "## Function to get an estimate of vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d8d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def estimate_vocab_size(corpus_path, min_n=2, max_n=5, min_ngram_freq=2):\n",
    "    \"\"\"\n",
    "    Estimate a suggested vocabulary size for a BPE tokenizer.\n",
    "\n",
    "    Args:\n",
    "        corpus_path (str): Path to the text corpus file.\n",
    "        min_n (int): Minimum character n-gram length.\n",
    "        max_n (int): Maximum character n-gram length.\n",
    "        min_ngram_freq (int): Minimum frequency of n-gram to be considered.\n",
    "\n",
    "    Returns:\n",
    "        dict: Suggested vocab sizes based on words and subwords.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read corpus\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    num_unique_words = len(unique_words)\n",
    "\n",
    "    # Count frequent character n-grams (for subword estimate)\n",
    "    ngrams = Counter()\n",
    "    for word in words:\n",
    "        for n in range(min_n, max_n+1):\n",
    "            for i in range(len(word)-n+1):\n",
    "                ngram = word[i:i+n]\n",
    "                ngrams[ngram] += 1\n",
    "\n",
    "    frequent_ngrams = [ng for ng, freq in ngrams.items() if freq >= min_ngram_freq]\n",
    "\n",
    "    return {\n",
    "        \"unique_words\": num_unique_words,\n",
    "        \"frequent_ngrams\": len(frequent_ngrams),\n",
    "        \"suggested_vocab_size\": num_unique_words + len(frequent_ngrams)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8286bd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 47\n",
      "Number of frequent subword n-grams: 59\n",
      "Suggested vocabulary size for BPE tokenizer: 106\n"
     ]
    }
   ],
   "source": [
    "corpus_file = \"hindi_corpus.txt\"\n",
    "vocab_stats = estimate_vocab_size(corpus_file)\n",
    "\n",
    "print(\"Number of unique words:\", vocab_stats[\"unique_words\"])\n",
    "print(\"Number of frequent subword n-grams:\", vocab_stats[\"frequent_ngrams\"])\n",
    "print(\"Suggested vocabulary size for BPE tokenizer:\", vocab_stats[\"suggested_vocab_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6ec83",
   "metadata": {},
   "source": [
    "## Train a BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "151bbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9463d6",
   "metadata": {},
   "source": [
    "### Initialize a BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaea3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775e93a",
   "metadata": {},
   "source": [
    "### Set pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5bbcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-tokenizer splits text into initial chunks based on whitespace. \n",
    "# E.g., \"भारत में\" → [\"भारत\", \"में\"]\n",
    "'''\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13180c89",
   "metadata": {},
   "source": [
    "### Setup trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afdf3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trainer controls how the tokenizer learns merges.\n",
    "vocab_size` = number of subwords it will learn.\n",
    "special_tokens` are reserved for unknown words or padding.\n",
    "'''\n",
    "\n",
    "\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=106,               # As suggested by estimate_vocab_size function\n",
    "    special_tokens=[\"<unk>\", \"<pad>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac90e9b1",
   "metadata": {},
   "source": [
    "### Train tokenizer on hindi corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c84283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\"hindi_corpus.txt\"]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564cfaf",
   "metadata": {},
   "source": [
    "### Save Custom hindi tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e169d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"custom-hindi-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23fe759",
   "metadata": {},
   "source": [
    "## Load custom BPE hindi tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf92485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"custom-hindi-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63a0ee",
   "metadata": {},
   "source": [
    "### Test encoding new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f10958",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"भारत में शिक्षा का महत्व\"\n",
    "output = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0afa69b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['भारत', 'मे', 'ं', 'श', 'ि', 'क्ष', 'ा', 'का', 'म', 'ह', 'त्', 'व']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens:\", output.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29181cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: [74, 68, 4, 31, 37, 65, 36, 87, 26, 34, 54, 30]\n"
     ]
    }
   ],
   "source": [
    "print(\"IDs:\", output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8ad24a",
   "metadata": {},
   "source": [
    "## Load with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85e29c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"custom-hindi-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32c50de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['भारत', 'मे', 'ं', 'श', 'ि', 'क्ष', 'ा', 'का', 'म', 'ह', 'त्', 'व']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"भारत में शिक्षा का महत्व\"\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef382bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids =  tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78a94433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[74, 68,  4, 31, 37, 65, 36, 87, 26, 34, 54, 30]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9591dcd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
